"""
AgentData core classes.

This module introduces the foundational AgentData class and its configuration
as the base for a future Knowledge Manager. It subclasses Langroid's
DocChatAgent and DocChatAgentConfig to stay aligned with existing agent
capabilities while remaining easy to extend.

References:
- Plan V12: ID 1.1 — Create AgentData Core Class
- Reuse Plan A1 — Foundational module for future reuse and extension
"""

from __future__ import annotations

import json
import tempfile
from datetime import UTC, datetime
from pathlib import Path

from agent_data.memory import FirestoreChatHistory

try:
    from google.api_core import exceptions  # type: ignore
    from google.cloud import storage  # type: ignore
except Exception:  # pragma: no cover - optional dependency at import time
    storage = None  # type: ignore
    exceptions = None  # type: ignore

# Optional Firestore import
try:
    from google.cloud import firestore  # type: ignore
except Exception:  # pragma: no cover - optional dependency at import time
    firestore = None  # type: ignore

from langroid.agent.special.doc_chat_agent import (
    DocChatAgent,
    DocChatAgentConfig,
)
from langroid.agent.tool_message import ToolMessage  # noqa: F401

try:  # prefer canonical location
    from langroid.agent.chat_agent import tool  # type: ignore
except Exception:  # pragma: no cover - compatibility shim for older Langroid

    def tool(func=None, *args, **kwargs):  # type: ignore
        def _wrap(f):
            return f

        return _wrap(func) if callable(func) else _wrap


__all__ = ["AgentDataConfig", "AgentData"]


class AgentDataConfig(DocChatAgentConfig):
    """Configuration for the AgentData agent.

    Currently identical to ``DocChatAgentConfig`` and serves as a placeholder
    for future, AgentData-specific options.

    - Plan V12 ID 1.1
    - Reuse Plan A1
    """


class AgentData(DocChatAgent):
    """AgentData: the core agent for the Knowledge Manager.

    This class extends Langroid's ``DocChatAgent`` and is intended to be the
    foundation for higher-level orchestration and capabilities. It is designed
    for straightforward future extension with AgentData-specific behaviors.

    - Plan V12 ID 1.1
    - Reuse Plan A1
    """

    # Firestore collection to store document metadata
    METADATA_COLLECTION = "metadata_test"

    def __init__(self, config: AgentDataConfig) -> None:
        """Initialize an AgentData instance with the provided configuration.

        Parameters
        ----------
        config:
            The configuration object for AgentData; currently the same shape as
            ``DocChatAgentConfig``.
        """

        super().__init__(config)

        # Load optional system prompts from prompts/ repository
        self.system_prompt: str | None = None
        self.summarization_prompt: str | None = None

        try:
            project_root = Path(__file__).resolve().parents[1]
            prompts_dir = project_root / "prompts"
            if prompts_dir.is_dir():
                rag_path = prompts_dir / "rag_system_prompt.md"
                if rag_path.exists():
                    self.system_prompt = rag_path.read_text(encoding="utf-8").strip()
                sum_path = prompts_dir / "summarization_prompt.md"
                if sum_path.exists():
                    self.summarization_prompt = sum_path.read_text(
                        encoding="utf-8"
                    ).strip()
        except Exception:
            # Non-fatal: continue without prompts if not present
            self.system_prompt = self.system_prompt or None
            self.summarization_prompt = self.summarization_prompt or None
        # Initialize Firestore client before setting up history backend
        self.db = None
        try:
            if firestore is not None:
                self.db = firestore.Client()  # type: ignore[attr-defined]
        except Exception:
            self.db = None

        # Integrate Firestore-backed chat history (lazy per-session)
        self.history = None

        # Track tool registrations for simple verification/tests.
        # In full Langroid integration, tool enablement is handled via
        # ToolMessage classes and enable_message(); here we maintain
        # a minimal list of tool names for skeleton validation.
        self.tools = getattr(self, "tools", []) or []
        if "gcs_ingest" not in self.tools:
            self.tools.append("gcs_ingest")
        # Keep a simple preview/cache of last ingested text content (for demo/tests)
        self.last_ingested_text: str | None = None

        # Firestore client already initialized above to support history backend

    # -------- Session helpers --------
    def set_session(self, session_id: str) -> None:
        """Bind this agent to a session-backed chat history if Firestore is available."""
        try:
            if self.db is not None:
                self.history = FirestoreChatHistory(
                    session_id=session_id, firestore_client=self.db
                )
            else:
                self.history = None
        except Exception:
            self.history = None

    def ingest_doc_paths(self, paths, *args, **kwargs) -> str:
        """Overrides the parent method to automatically persist metadata to Firestore after successful ingestion.

        Calls the parent ingestion to process the documents, then attempts to
        persist initial metadata per input path. Errors during metadata
        persistence are swallowed so they don't affect ingestion success.

        Returns:
            str: A summary message including ingestion result and metadata outcome.
        """

        # Execute the standard ingestion (vector store, parsing, etc.)
        try:
            parent_result = super().ingest_doc_paths(paths, *args, **kwargs)
        except Exception as e:  # Always persist metadata even if parent ingestion fails
            parent_result = f"Ingestion skipped or failed: {e}"

        # Normalize paths into a list of strings (ignore bytes for metadata doc_id)
        # Accept either a single path or an iterable of paths
        if isinstance(paths, str | bytes):
            norm_paths = [paths]
        else:
            norm_paths = list(paths)

        saved_ids: list[str] = []
        errors: list[str] = []

        for p in norm_paths:
            if isinstance(p, bytes):
                # Cannot infer an id from raw bytes; skip metadata persistence
                continue
            try:
                doc_id = Path(p).name
                initial_metadata = {
                    "source_uri": p,
                    "ingestion_status": "completed",
                    "timestamp_utc": datetime.now(UTC).isoformat(),
                }
                _ = self.add_metadata(doc_id, json.dumps(initial_metadata))
                saved_ids.append(doc_id)
            except Exception as e:  # pragma: no cover
                errors.append(f"{p}: {e}")

        meta_note = (
            f"metadata saved for {len(saved_ids)} doc(s): {', '.join(saved_ids)}"
            if saved_ids
            else "no metadata saved"
        )
        if errors:
            meta_note += f"; {len(errors)} error(s) during metadata save"

        return f"Ingestion complete. Result: {parent_result}. Metadata: {meta_note}."

    @tool
    def gcs_ingest(self, gcs_uri: str) -> str:
        """Downloads a file from the given GCS URI, ingests it into the vector store.

        Args:
            gcs_uri (str): The GCS URI of the file to ingest, e.g.,
                "gs://bucket-name/path/to/file.pdf".

        Returns:
            str: A confirmation message indicating the result of the ingestion.
        """

        # Validate availability of Google Cloud Storage client
        if storage is None or exceptions is None:  # pragma: no cover
            return (
                "GCS client libraries not available. Please install 'google-cloud-storage' "
                f"and try again. URI: {gcs_uri}"
            )

        def parse_gcs_uri(uri: str) -> tuple[str, str]:
            if not uri.startswith("gs://"):
                raise ValueError(
                    f"Invalid GCS URI '{uri}'. Expected format: gs://<bucket>/<path>"
                )
            path = uri[len("gs://") :]
            if "/" not in path:
                raise ValueError(
                    f"Invalid GCS URI '{uri}'. Missing object path after bucket name."
                )
            bucket_name, blob_name = path.split("/", 1)
            if not bucket_name or not blob_name:
                raise ValueError(
                    f"Invalid GCS URI '{uri}'. Bucket and object path must be non-empty."
                )
            return bucket_name, blob_name

        try:
            bucket_name, blob_name = parse_gcs_uri(gcs_uri)

            client = storage.Client()
            bucket = client.bucket(bucket_name)
            blob = bucket.blob(blob_name)

            with tempfile.TemporaryDirectory(prefix="agentdata_gcs_") as tmpdir:
                # Save to a simple filename inside the temporary directory
                filename = Path(blob_name).name or "downloaded_object"
                local_path = Path(tmpdir) / filename
                blob.download_to_filename(str(local_path))

                # After download, integrate with DocChatAgent ingestion.
                ingestion_result = None
                try:
                    ingestion_result = self.ingest_doc_paths([str(local_path)])
                except (
                    Exception
                ) as ingest_err:  # pragma: no cover - integration fallback
                    ingestion_result = f"Ingestion skipped or failed: {ingest_err}"

                # Best-effort: cache text for simple QA when vecdb/LLM are absent
                try:
                    text = local_path.read_text(encoding="utf-8", errors="ignore")
                    self.last_ingested_text = text[:10000]
                except Exception:
                    pass

                # Note: local_path resides in a TemporaryDirectory and will be
                # cleaned up on context exit; we return the ingestion result
                # for confirmation in logs or tooling.
                return f"Successfully downloaded and ingested {gcs_uri}. Result: {ingestion_result}"

        except Exception as e:
            # Handle common GCS exceptions with clearer messages
            if exceptions is not None and isinstance(e, exceptions.NotFound):
                return f"File not found: {gcs_uri}"
            if exceptions is not None and isinstance(e, exceptions.Forbidden):
                return (
                    "Access forbidden when attempting to download from GCS. "
                    f"Check permissions for URI: {gcs_uri}"
                )
            if exceptions is not None and isinstance(e, exceptions.GoogleAPICallError):
                return f"GCS API error for URI {gcs_uri}: {e}"
            # Fallback for any other error
            return f"Failed to download from GCS for URI {gcs_uri}: {e}"

    @tool
    def add_metadata(self, document_id: str, metadata_json: str) -> str:
        """Adds or overwrites metadata for a given document ID in Firestore.

        Args:
            document_id (str): The unique identifier for the document.
            metadata_json (str): A JSON string representing the metadata to add.
        Returns:
            str: A confirmation message or error detail.
        """

        if "add_metadata" not in self.tools:
            self.tools.append("add_metadata")

        if self.db is None:
            return "Firestore client not initialized."
        try:
            data = json.loads(metadata_json)
        except Exception as e:  # pragma: no cover
            return f"Invalid metadata JSON: {e}"

        try:
            self.db.collection(self.METADATA_COLLECTION).document(document_id).set(data)
            return f"Metadata for {document_id} saved."
        except Exception as e:  # pragma: no cover
            return f"Failed to add metadata for {document_id}: {e}"

    @tool
    def get_metadata(self, document_id: str) -> str:
        """Retrieves the metadata for a given document ID from Firestore.

        Args:
            document_id (str): The unique identifier for the document to retrieve.
        Returns:
            str: A JSON string of the metadata, or an error message.
        """

        if "get_metadata" not in self.tools:
            self.tools.append("get_metadata")

        if self.db is None:
            return "Firestore client not initialized."
        try:
            doc_ref = self.db.collection(self.METADATA_COLLECTION).document(document_id)
            doc = doc_ref.get()
            if getattr(doc, "exists", False):
                try:
                    return json.dumps(doc.to_dict())
                except Exception:
                    return str(doc.to_dict())
            else:
                return f"Metadata not found for {document_id}."
        except Exception as e:  # pragma: no cover
            return f"Failed to get metadata for {document_id}: {e}"

    @tool
    def update_ingestion_status(self, document_id: str, status: str) -> str:
        """Updates the ingestion status for a document in Firestore.

        Args:
            document_id (str): The unique identifier for the document.
            status (str): The new status (e.g., 'pending', 'completed', 'failed').
        Returns:
            str: A confirmation message or error detail.
        """

        if "update_ingestion_status" not in self.tools:
            self.tools.append("update_ingestion_status")

        if self.db is None:
            return "Firestore client not initialized."
        try:
            self.db.collection(self.METADATA_COLLECTION).document(document_id).update(
                {"ingestion_status": status}
            )
            return f"Ingestion status for {document_id} updated to '{status}'."
        except Exception as e:  # pragma: no cover
            return f"Failed to update status for {document_id}: {e}"
